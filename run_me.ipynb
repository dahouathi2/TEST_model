{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.timefeatures import time_features\n",
    "from ean_global_channel import check_saved_standardization_data, generate_standardization_dicts, save_standardization_data, load_standardization_data\n",
    "import warnings\n",
    "import hashlib\n",
    "from torch.utils.data import DataLoader\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Dataset_Promo_ean_global_channel(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='ETTh1.csv',\n",
    "                 target='sold_units', scale=False, inverse=False, timeenc=0, freq='15min',\n",
    "                 seasonal_patterns='Yearly', scale_path=None, embedding=True, embedding_dimension = 2):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.scale_path = scale_path\n",
    "        self.inverse = inverse\n",
    "        self.timeenc = timeenc\n",
    "        self.root_path = root_path\n",
    "\n",
    "        self.embedding_dict = {}\n",
    "\n",
    "        self.seq_len = size[0]\n",
    "        self.label_len = size[1]\n",
    "        self.pred_len = size[2]\n",
    "        self.embedding_dim = embedding_dimension\n",
    "        self.embedding = embedding\n",
    "        self.seasonal_patterns = seasonal_patterns\n",
    "        self.history_size = 2\n",
    "        self.window_sampling_limit = int(self.history_size * self.pred_len)\n",
    "        self.flag = flag\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def generate_combinations(self, n):\n",
    "        \"\"\"Generates all unique combinations of binary values for n binary columns\"\"\"\n",
    "        return [[(i >> j) & 1 for j in range(n)] for i in range(2**n)]\n",
    "    def deterministic_embedding(self, comb):\n",
    "        \"\"\"Generates a deterministic embedding based on a hash of the combination\"\"\"\n",
    "        hash_object = hashlib.sha256(str(comb).encode())\n",
    "        hash_digest = hash_object.digest()\n",
    "        seed = int.from_bytes(hash_digest[:4], 'little')\n",
    "        rng = np.random.default_rng(seed)\n",
    "        return rng.random(self.embedding_dim)\n",
    "\n",
    "    def preprocess_pipeline(self, data, id='ean_global_channel'):\n",
    "        \"\"\"This function is responsible for all the preprocessing \"\"\"\n",
    "        data = data.rename(columns={'end_date': 'date', id: 'id'})\n",
    "        data = data.drop(['is_promo', 'sub_axis', 'year', 'month', 'week'], axis=1)\n",
    "        cols = list(data.columns)\n",
    "        cols.remove(self.target)\n",
    "        cols.remove('date')\n",
    "        data = data[cols + [self.target]]  # organize data to date, variables and last is target we're not using date now\n",
    "\n",
    "        binary_columns = [col for col in data.columns if col not in ['price_range', 'seasonality_index', 'id', self.target]]\n",
    "\n",
    "        unique_combinations = self.generate_combinations(len(binary_columns))\n",
    "        self.embedding_dict = {tuple(comb): self.deterministic_embedding(comb) for comb in unique_combinations}\n",
    "        \n",
    "        def get_embedding(row):\n",
    "            comb = tuple(row[binary_columns])\n",
    "            return self.embedding_dict[comb]\n",
    "        if self.embedding:\n",
    "            embeddings = data[binary_columns].apply(get_embedding, axis=1)\n",
    "            embedding_df = pd.DataFrame(embeddings.tolist(), columns=['embedding_1', 'embedding_2'])\n",
    "            data = pd.concat([data, embedding_df], axis=1)\n",
    "            data = data.drop(columns=binary_columns)\n",
    "        if self.scale:\n",
    "            columns_to_standarize = ['price_range', 'sold_units', 'seasonality_index']\n",
    "            if not check_saved_standardization_data(self.scale_path):\n",
    "                mean_dict, std_dict, ids = generate_standardization_dicts(data)\n",
    "                save_standardization_data(mean_dict, std_dict, ids, self.scale_path)\n",
    "                print(f\"standarization dictionaries are created in{self.scale_path}\")\n",
    "            print(f\"scaling the data of {self.flag}\")\n",
    "            mean_dict, std_dict, ids = load_standardization_data(self.scale_path)\n",
    "            standardized_data = pd.DataFrame()\n",
    "            for id_value, group in data.groupby('id'):\n",
    "                if id_value in mean_dict:\n",
    "                    means = pd.Series(mean_dict[id_value])\n",
    "                    stds = pd.Series(std_dict[id_value])\n",
    "                    standardized_group = group.copy()\n",
    "                    for col in columns_to_standarize:\n",
    "                        if col in means and col in stds:\n",
    "                            # Standardize each column in the group using the training set stats\n",
    "                            standardized_group[col] = (group[col] - means[col]) / stds[col]\n",
    "                        else:\n",
    "                            print(f\"No training data statistics for column: {col} in id: {id_value}. Skipping standardization for this column.\")\n",
    "                    standardized_group['id'] = id_value  # Add id column back\n",
    "                    standardized_data = pd.concat([standardized_data, standardized_group])\n",
    "                else:\n",
    "                    print(f\"No training data statistics for id: {id_value}. Skipping standardization for this id.\")\n",
    "            print(f\"standarization is over of {self.flag}\")\n",
    "        else:\n",
    "            standardized_data = data.copy()\n",
    "        cols = list(standardized_data.columns)\n",
    "        cols.remove(self.target)\n",
    "        standardized_data = standardized_data[cols + [self.target]]\n",
    "        return standardized_data\n",
    "        \n",
    "    def __read_data__(self):\n",
    "        if self.flag == 'train':\n",
    "            dataset = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path)) \n",
    "        else:\n",
    "            dataset = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path.replace('train', 'test')))\n",
    "        # Preprocessing dataset:\n",
    "        df = self.preprocess_pipeline(dataset)\n",
    "        self.ids = df['id'].unique()#[:100]\n",
    "        self.timeseries = [df[df['id']==self.ids[i]].drop('id', axis=1).values for i in range(len(self.ids))]\n",
    "        self.n_var = self.timeseries[0].shape[1]\n",
    "    def __getitem__(self, index):\n",
    "        insample = np.zeros((self.seq_len, self.n_var))\n",
    "        insample_mask = np.zeros((self.seq_len, self.n_var))\n",
    "        outsample = np.zeros((self.pred_len + self.label_len, self.n_var))\n",
    "        outsample_mask = np.zeros((self.pred_len + self.label_len, self.n_var))  # m4 dataset\n",
    "\n",
    "        sampled_timeseries = self.timeseries[index]\n",
    "        # cut_point = np.random.randint(low=max(1, len(sampled_timeseries) - self.window_sampling_limit),\n",
    "        #                               high=len(sampled_timeseries),\n",
    "        #                               size=1)[0]\n",
    "        if self.flag=='train':\n",
    "            cut_point = np.random.randint(low=self.seq_len,\n",
    "                                      high=len(sampled_timeseries)-self.pred_len+1,\n",
    "                                      size=1)[0]\n",
    "        else:\n",
    "            cut_point = np.random.randint(low=max(1, len(sampled_timeseries)- self.window_sampling_limit),\n",
    "                                      high=len(sampled_timeseries),\n",
    "                                      size=1)[0]\n",
    "            # if self.flag =='train':\n",
    "            #     print(cut_point)\n",
    "        # cut_point = np.random.randint(low=self.seq_len,\n",
    "        #                               high=len(sampled_timeseries),\n",
    "        #                               size=1)[0]\n",
    "        insample_window = sampled_timeseries[max(0, cut_point - self.seq_len):cut_point]\n",
    "        insample[-len(insample_window):] = insample_window\n",
    "        insample_mask[-len(insample_window):] = 1.0\n",
    "        outsample_window = sampled_timeseries[\n",
    "                           cut_point - self.label_len:min(len(sampled_timeseries), cut_point + self.pred_len)]\n",
    "        outsample[:len(outsample_window)] = outsample_window\n",
    "        outsample_mask[:len(outsample_window)] = 1.0\n",
    "        return insample, outsample, insample_mask, outsample_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.timeseries)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "    def last_insample_window(self):\n",
    "        \"\"\"\n",
    "        The last window of insample size of all timeseries.\n",
    "        This function does not support batching and does not reshuffle timeseries.\n",
    "\n",
    "        :return: Last insample window of all timeseries. Shape \"timeseries, insample size\"\n",
    "        \"\"\"\n",
    "        insample = np.zeros((len(self.timeseries), self.seq_len, self.n_var))\n",
    "        insample_mask = np.zeros((len(self.timeseries), self.seq_len, self.n_var))\n",
    "        for i, ts in enumerate(self.timeseries):\n",
    "            ts_last_window = ts[-self.seq_len:]\n",
    "            insample[i, -len(ts):] = ts_last_window\n",
    "            insample_mask[i, -len(ts):] = 1.0\n",
    "        return insample, insample_mask\n",
    "\n",
    "def data_provider(args, flag):\n",
    "    timeenc = 0 if args.embed != 'timeF' else 1\n",
    "    percent = args.percent\n",
    "\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = True\n",
    "        batch_size = args.batch_size\n",
    "        freq = args.freq\n",
    "    else:\n",
    "        shuffle_flag = True\n",
    "        drop_last = False\n",
    "        batch_size = args.batch_size\n",
    "        freq = args.freq\n",
    "\n",
    "    data_set = Dataset_Promo_ean_global_channel(\n",
    "        root_path=args.root_path,\n",
    "        data_path=args.data_path,\n",
    "        flag=flag,\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        scale=args.scale,\n",
    "        scale_path=args.scale_path,\n",
    "        embedding=args.embedding,\n",
    "        embedding_dimension=args.embedding_dimension\n",
    "    )\n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=drop_last)\n",
    "    return data_set, data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import LlamaConfig, LlamaModel, LlamaTokenizer, GPT2Config, GPT2Model, GPT2Tokenizer, BertConfig, \\\n",
    "    BertModel, BertTokenizer\n",
    "from layers.Embed import PatchEmbedding\n",
    "import transformers\n",
    "from layers.StandardNorm import Normalize\n",
    "from vertexai.preview import VertexModel # VertexModel\n",
    "import vertexai\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, vali, load_content\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "def test_MS(args, device, model, train_loader, vali_loader, criterion):\n",
    "    x, _ = train_loader.dataset.last_insample_window()\n",
    "    y = vali_loader.dataset.timeseries\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "    print(\"Shape of X eval\", x.shape)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B, _, C = x.shape\n",
    "        dec_inp = torch.zeros((B, args.pred_len, C)).float().to(device)\n",
    "        dec_inp = torch.cat([x[:, -args.label_len:, :], dec_inp], dim=1)\n",
    "        outputs = torch.zeros((B, args.pred_len, C)).float().to(device)\n",
    "        id_list = np.arange(0, B, args.eval_batch_size)\n",
    "        id_list = np.append(id_list, B)\n",
    "        with autocast():\n",
    "            for i in range(len(id_list) - 1):\n",
    "                outputs[id_list[i]:id_list[i + 1], :, :] = model(\n",
    "                    x[id_list[i]:id_list[i + 1]],\n",
    "                    None,\n",
    "                    dec_inp[id_list[i]:id_list[i + 1]],\n",
    "                    None\n",
    "                )\n",
    "        print(\"Shape of output eval before choosing\", outputs.shape)\n",
    "        f_dim = -1 if args.features == 'MS' else 0\n",
    "        outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "        pred = outputs\n",
    "        true = torch.from_numpy(np.array(y)).to(device)\n",
    "        true = true[:, -args.pred_len:, f_dim:]\n",
    "        print(\"Shape of y eval\", true.shape)\n",
    "        batch_y_mark = torch.ones(true.shape).to(device)\n",
    "\n",
    "        loss = criterion(pred, true)\n",
    "\n",
    "    model.train()\n",
    "    return loss\n",
    "\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module,VertexModel):\n",
    "\n",
    "    def __init__(self, configs, patch_len=16, stride=8):\n",
    "        nn.Module.__init__(self)\n",
    "        VertexModel.__init__(self)\n",
    "        self.task_name = configs.task_name\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.top_k = 5\n",
    "        self.d_llm = configs.llm_dim\n",
    "        self.patch_len = configs.patch_len\n",
    "        self.stride = configs.stride\n",
    "        self.args = configs\n",
    "\n",
    "        if configs.llm_model == 'LLAMA':\n",
    "            # self.llama_config = LlamaConfig.from_pretrained('/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/')\n",
    "            self.llama_config = LlamaConfig.from_pretrained('huggyllama/llama-7b')\n",
    "            self.llama_config.num_hidden_layers = configs.llm_layers\n",
    "            self.llama_config.output_attentions = True\n",
    "            self.llama_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            try:\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'GPT2':\n",
    "            self.gpt2_config = GPT2Config.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "            self.gpt2_config.num_hidden_layers = configs.llm_layers\n",
    "            self.gpt2_config.output_attentions = True\n",
    "            self.gpt2_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'BERT':\n",
    "            self.bert_config = BertConfig.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "            self.bert_config.num_hidden_layers = configs.llm_layers\n",
    "            self.bert_config.output_attentions = True\n",
    "            self.bert_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        else:\n",
    "            raise Exception('LLM model is not defined')\n",
    "\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            pad_token = '[PAD]'\n",
    "            self.tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "            self.tokenizer.pad_token = pad_token\n",
    "\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if configs.prompt_domain:\n",
    "            self.description = configs.content\n",
    "        else:\n",
    "            self.description = 'The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment.'\n",
    "\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            configs.d_model, self.patch_len, self.stride, configs.dropout)\n",
    "\n",
    "        self.word_embeddings = self.llm_model.get_input_embeddings().weight\n",
    "        self.vocab_size = self.word_embeddings.shape[0]\n",
    "        self.num_tokens = 1000\n",
    "        self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)\n",
    "\n",
    "        self.reprogramming_layer = ReprogrammingLayer(configs.d_model, configs.n_heads, self.d_ff, self.d_llm)\n",
    "\n",
    "        self.patch_nums = int((configs.seq_len - self.patch_len) / self.stride + 2)\n",
    "        self.head_nf = self.d_ff * self.patch_nums\n",
    "\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            self.output_projection = FlattenHead(configs.enc_in, self.head_nf, self.pred_len,\n",
    "                                                 head_dropout=configs.dropout)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.normalize_layers = Normalize(configs.enc_in, affine=False)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            return dec_out[:, -self.pred_len:, :]\n",
    "        return None\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "\n",
    "        x_enc = self.normalize_layers(x_enc, 'norm')\n",
    "\n",
    "        B, T, N = x_enc.size()\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "\n",
    "        min_values = torch.min(x_enc, dim=1)[0]\n",
    "        max_values = torch.max(x_enc, dim=1)[0]\n",
    "        medians = torch.median(x_enc, dim=1).values\n",
    "        lags = self.calcute_lags(x_enc)\n",
    "        trends = x_enc.diff(dim=1).sum(dim=1)\n",
    "\n",
    "        prompt = []\n",
    "        for b in range(x_enc.shape[0]):\n",
    "            min_values_str = str(min_values[b].tolist()[0])\n",
    "            max_values_str = str(max_values[b].tolist()[0])\n",
    "            median_values_str = str(medians[b].tolist()[0])\n",
    "            lags_values_str = str(lags[b].tolist())\n",
    "            prompt_ = (\n",
    "                f\"<|start_prompt|>Dataset description: {self.description}\"\n",
    "                f\"Task description: forecast the next {str(self.pred_len)} steps given the previous {str(self.seq_len)} steps information; \"\n",
    "                \"Input statistics: \"\n",
    "                f\"min value {min_values_str}, \"\n",
    "                f\"max value {max_values_str}, \"\n",
    "                f\"median value {median_values_str}, \"\n",
    "                f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n",
    "                f\"top 5 lags are : {lags_values_str}<|<end_prompt>|>\"\n",
    "            )\n",
    "\n",
    "            prompt.append(prompt_)\n",
    "\n",
    "        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous() # B, T, N\n",
    "\n",
    "        prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n",
    "\n",
    "        source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous() # B N T\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc.to(torch.bfloat16))\n",
    "        enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n",
    "        llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "        dec_out = dec_out[:, :, :self.d_ff]\n",
    "\n",
    "        dec_out = torch.reshape(\n",
    "            dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1]))\n",
    "        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums:])\n",
    "        dec_out = dec_out.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dec_out = self.normalize_layers(dec_out, 'denorm')\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "    @vertexai.preview.developer.mark.train()\n",
    "    def train_model(self, train_loader, vali_loader, criterion, path):\n",
    "        torch.set_printoptions(profile=\"full\")\n",
    "        import torch.multiprocessing as mp\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "        # for name, param in self.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         if name == 'mapping_layer.weight' or name == 'reprogramming_layer.query_projection.weight':\n",
    "        #             print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:1]} \\n\")\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        self.args.content = load_content(self.args)\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(accelerator=None, patience=self.args.patience)\n",
    "\n",
    "        trained_parameters = [p for p in self.parameters() if p.requires_grad]\n",
    "        model_optim = optim.Adam(trained_parameters, lr=self.args.learning_rate)\n",
    "\n",
    "        if self.args.lradj == 'COS':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "        else:\n",
    "            scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                                steps_per_epoch=train_steps,\n",
    "                                                pct_start=self.args.pct_start,\n",
    "                                                epochs=self.args.train_epochs,\n",
    "                                                max_lr=self.args.learning_rate)\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(device)\n",
    "        print(f'device{device}')\n",
    "        scaler = GradScaler()\n",
    "        accumulation_steps = 4\n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "\n",
    "            self.train()\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "                batch_x = batch_x.float().to(device)\n",
    "                batch_y = batch_y.float().to(device)\n",
    "                batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float().to(device)\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "                \n",
    "                with autocast():\n",
    "                    outputs = self(batch_x, None, dec_inp, None)\n",
    "\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y_mark = batch_y_mark[:, -self.args.pred_len:, f_dim:]\n",
    "\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss = loss / accumulation_steps\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    scaler.step(model_optim)\n",
    "                    scaler.update()\n",
    "                    model_optim.zero_grad()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(\n",
    "                        \"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item())\n",
    "                    )\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                if self.args.lradj == 'TST':\n",
    "                    adjust_learning_rate(None, model_optim, scheduler, epoch + 1, self.args, printout=False)\n",
    "                    scheduler.step()\n",
    "\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            print('########################################################################')\n",
    "            vali_loss = test_MS(self.args, device, self, train_loader, vali_loader, criterion)\n",
    "            test_loss = vali_loss\n",
    "            print(\n",
    "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            \n",
    "            early_stopping(vali_loss, self, path)  # model saving\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            if self.args.lradj != 'TST':\n",
    "                adjust_learning_rate(None, model_optim, scheduler, epoch + 1, self.args, printout=True)\n",
    "            else:\n",
    "                print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "        \n",
    "        torch.save(self.state_dict(), './checkpoint')\n",
    "\n",
    "    \n",
    "    def calcute_lags(self, x_enc):\n",
    "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "        mean_value = torch.mean(corr, dim=1)\n",
    "        _, lags = torch.topk(mean_value, self.top_k, dim=-1)\n",
    "        return lags\n",
    "\n",
    "\n",
    "\n",
    "class ReprogrammingLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):\n",
    "        super(ReprogrammingLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, _ = target_embedding.shape\n",
    "        S, _ = source_embedding.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)\n",
    "        source_embedding = self.key_projection(source_embedding).view(S, H, -1)\n",
    "        value_embedding = self.value_projection(value_embedding).view(S, H, -1)\n",
    "\n",
    "        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n",
    "\n",
    "        out = out.reshape(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)\n",
    "\n",
    "    def reprogramming(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, H, E = target_embedding.shape\n",
    "\n",
    "        scale = 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n",
    "\n",
    "        return reprogramming_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.task_name = 'short_term_forecast'\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'promo_ean_channel'\n",
    "        self.model_comment = 'pretrain-EAN_Channel'\n",
    "        self.model = 'TimeLLM'\n",
    "        self.seed = 2021\n",
    "        self.data = 'promo_ean_channel'\n",
    "        self.root_path = './dataset'\n",
    "        self.data_path = 'train.csv'\n",
    "        self.features = 'MS'\n",
    "        self.target = 'sold_units'\n",
    "        self.loader = 'modal'\n",
    "        self.freq = 'h'\n",
    "        self.checkpoints = './checkpoints/'\n",
    "\n",
    "        self.seasonal_patterns = 'Monthly'\n",
    "        self.moving_avg = 1\n",
    "        self.dropout = 0.1\n",
    "        self.embed = 'timeF'\n",
    "        self.activation = 'gelu'\n",
    "        self.output_attention = False\n",
    "        self.prompt_domain = 0\n",
    "        self.llm_model = 'GPT2'\n",
    "        self.llm_dim = 768\n",
    "        self.num_workers = 10\n",
    "        self.itr = 1\n",
    "        \n",
    "        self.align_epochs = 10\n",
    "        self.batch_size = 1\n",
    "        self.eval_batch_size = 1\n",
    "        \n",
    "        \n",
    "        self.des = 'Exp'\n",
    "        \n",
    "        self.lradj = 'type1'\n",
    "        self.pct_start = 0.2\n",
    "        self.use_amp = False\n",
    "        self.percent = 100\n",
    "\n",
    "        self.train_epochs = 2\n",
    "        self.patience = 20\n",
    "        self.learning_rate = 100\n",
    "        self.loss = 'MSE'\n",
    "\n",
    "        self.zero_percent = 0\n",
    "        self.interpolation = False\n",
    "        self.interpolation_method = False\n",
    "        self.fill_discontinuity = True\n",
    "        self.month = 11\n",
    "        self.num_weeks = 48\n",
    "        self.scale = True\n",
    "        self.embedding = True\n",
    "        self.embedding_dimension = 2\n",
    "        self.keep_non_promo = False\n",
    "        self.channel = 'Offline'\n",
    "        self.pretrain = False\n",
    "        \n",
    "        self.seq_len = 34\n",
    "        self.label_len = 17\n",
    "        self.pred_len = 17\n",
    "        self.patch_len = 1\n",
    "        self.stride = 1\n",
    "        self.enc_in = 7\n",
    "        self.dec_in = 7\n",
    "        self.c_out = 7\n",
    "        self.d_model = 16\n",
    "        self.llm_layers = 16\n",
    "        self.n_heads = 8\n",
    "        self.e_layers = 2\n",
    "        self.d_layers = 1\n",
    "        self.d_ff = 32\n",
    "        self.factor = 1\n",
    "# Instantiate the Args\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from gcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.losses import smape_loss\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, load_content\n",
    "\n",
    "from ean_global_channel import import_true_promo, import_all, check_saved_standardization_data, delete_saved_standardization_data\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(2.0 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100\n",
    "\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ID = \"itg-bpma-gbl-ww-np\"  # @param {type:\"string\"}\n",
    "REGION = \"europe-west1\" \n",
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    "import vertexai\n",
    "REMOTE_JOB_NAME = \"timeseriesllm1\"\n",
    "REMOTE_JOB_BUCKET = f\"{BUCKET_URI}/{REMOTE_JOB_NAME}\"\n",
    "##################################################################################################\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=REMOTE_JOB_BUCKET,\n",
    ")\n",
    "\n",
    "##################################################################################################\n",
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,  # GCP project used for running the queries \n",
    ")\n",
    "\n",
    "#################################################################################################\n",
    "pred_len = 17\n",
    "\n",
    "#print(\"Ended up with \", 4*pred_len)\n",
    "args.num_weeks=4*pred_len\n",
    "args.pred_len = pred_len\n",
    "args.label_len = pred_len\n",
    "args.seq_len = 2*pred_len\n",
    "# print(f\"{args.seq_len}\")\n",
    "#print(\"Let's Load the Data From GCP\")\n",
    "# if args.interpolation:\n",
    "#     final_data, train_set, test_set, pred_len = import_all(\n",
    "#         client=bq_client,\n",
    "#         zero_percent=args.zero_percent,\n",
    "#         month=args.month,\n",
    "#         num_weeks=args.num_weeks,\n",
    "#         channel=args.channel,\n",
    "#         fill_discontinuity=args.fill_discontinuity,\n",
    "#         keep_non_promo=args.keep_non_promo,\n",
    "#         interpolation_method=args.interpolation_method\n",
    "#     )\n",
    "# else :\n",
    "#     final_data, train_set, test_set, pred_len = import_true_promo(\n",
    "#         client=bq_client,\n",
    "#         zero_percent=args.zero_percent,\n",
    "#         month=args.month,\n",
    "#         num_weeks=args.num_weeks,\n",
    "#         channel=args.channel,\n",
    "#         fill_discontinuity=args.fill_discontinuity,\n",
    "#         keep_non_promo=args.keep_non_promo\n",
    "#     )\n",
    "\n",
    "setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.des)\n",
    "################## \n",
    "# Construct the path\n",
    "base_dir = f\"dataset/\"\n",
    "if args.interpolation:\n",
    "    base_dir += f\"interpolation_{args.interpolation_method}/\"\n",
    "else : \n",
    "    base_dir += f\"true_promo/\"\n",
    "\n",
    "base_dir+= f\"{args.channel}Channel_Month{args.month}_{args.num_weeks}Weeks\"\n",
    "if args.fill_discontinuity:\n",
    "    base_dir += \"_filldiscont\"\n",
    "if args.keep_non_promo:\n",
    "    base_dir += \"_keepnonpromo\"\n",
    "if args.scale:\n",
    "    base_dir+=\"_scaled\"\n",
    "if args.embedding:\n",
    "    base_dir+=f\"_embedding_{args.embedding_dimension}\"\n",
    "base_dir += '/'+setting\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(base_dir, \"train.csv\")\n",
    "test_path = os.path.join(base_dir, \"test.csv\")\n",
    "\n",
    "\n",
    "train_set = pd.read_csv(train_path)\n",
    "test_set = pd.read_csv(test_path)\n",
    "\n",
    "if args.scale:\n",
    "     \n",
    "    args.scale_path = 'scale_path/' + base_dir[8:]\n",
    "    if check_saved_standardization_data(args.scale_path):\n",
    "        delete_saved_standardization_data(args.scale_path)\n",
    "\n",
    "\n",
    "########################################################### configuration ####################\n",
    "args.pred_len = pred_len\n",
    "args.label_len = pred_len\n",
    "args.seq_len = int(2*pred_len)\n",
    "args.root_path = base_dir\n",
    "args.data_path = 'train.csv'\n",
    "##############################################################################################\n",
    "#print(f\"{args.seq_len}\")\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.des, ii)\n",
    "\n",
    "    \n",
    "    path = os.path.join(args.checkpoints,\n",
    "                        base_dir[8:] + '_' + str(ii) + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    args.content = load_content(args)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standarization dictionaries are created inscale_path/true_promo/OfflineChannel_Month11_68Weeks_filldiscont_scaled_embedding_2/short_term_forecast_promo_ean_channel_TimeLLM_promo_ean_channel_ftMS_sl34_ll17_pl17_dm16_nh8_el2_dl1_df32_fc1_ebtimeF_Exp\n",
      "scaling the data of train\n",
      "standarization is over of train\n",
      "scaling the data of test\n",
      "standarization is over of test\n"
     ]
    }
   ],
   "source": [
    "train_data, train_loader = data_provider(args, 'train')\n",
    "test_data, test_loader = data_provider(args, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change le and epochs! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.learning_rate = 0.01\n",
    "args.train_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "model = Model(args).float()\n",
    "# Initialize optimizer with these parameters\n",
    "criterion = nn.MSELoss()\n",
    "if args.pretrain:\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}_{}'.format(\n",
    "    args.task_name,\n",
    "    'pretrain',\n",
    "    args.model,\n",
    "    'pretrain',\n",
    "    args.features,\n",
    "    args.seq_len,\n",
    "    args.label_len,\n",
    "    args.pred_len,\n",
    "    args.d_model,\n",
    "    args.n_heads,\n",
    "    args.e_layers,\n",
    "    args.d_layers,\n",
    "    args.d_ff,\n",
    "    args.factor,\n",
    "    args.embed,\n",
    "    args.des, ii)\n",
    "    pretrain_path = os.path.join(args.checkpoints,\n",
    "                    'pretrain/' + setting+'/checkpoint_pretrain')\n",
    "    if not os.path.exists(pretrain_path):\n",
    "        raise \"can't find pretrained path\"\n",
    "    model.load_state_dict(torch.load(pretrain_path), strict=False)\n",
    "    print(f\"pretrained model was loaded from{pretrain_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote job created. View the job: https://console.cloud.google.com/ai/platform/locations/europe-west1/training/6728740269811826688?project=238069609727\n"
     ]
    }
   ],
   "source": [
    "vertexai.preview.init(remote=True)\n",
    "model.train_model.vertex.remote_config.container_uri = \"europe-west1-docker.pkg.dev/itg-bpma-gbl-ww-np/timeseriesforecasting/torch-train:latest\"\n",
    "model.train_model.vertex.remote_config.enable_cuda = True\n",
    "model.train_model.vertex.remote_config.accelerator_count = 4\n",
    "model.train_model(train_loader, test_loader, criterion, path)\n",
    "torch.save(model.state_dict(),  path + '/' + 'checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: mapping_layer.weight | Size: torch.Size([1000, 50257]) | Values : tensor([[0.0032, 0.0033, 0.0024,  ..., 0.0037, 0.0007, 0.0014]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: reprogramming_layer.query_projection.weight | Size: torch.Size([256, 16]) | Values : tensor([[ 0.0243,  0.0304,  0.2257, -0.0924, -0.2241, -0.0075,  0.1639, -0.0313,\n",
      "          0.1483,  0.1348, -0.1936,  0.1491, -0.1729, -0.1193, -0.1242,  0.2027]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if name == 'mapping_layer.weight' or name == 'reprogramming_layer.query_projection.weight':\n",
    "            print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:1]} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check model initial params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: reprogramming_layer.query_projection.weight | Size: torch.Size([256, 16]) | Values : tensor([[ 0.0243,  0.0304,  0.2257, -0.0924, -0.2241, -0.0075,  0.1639, -0.0313,\n",
      "          0.1483,  0.1348, -0.1936,  0.1491, -0.1729, -0.1193, -0.1242,  0.2027]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: output_projection.linear.bias | Size: torch.Size([17]) | Values : tensor([-0.0210], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "for name, param in Model(args).float().named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if name == 'output_projection.linear.bias' or name == 'reprogramming_layer.query_projection.weight':\n",
    "            print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:1]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vertex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
